# План доработок проекта под требования устной сдачи

## 0) Цель проекта (как формулировать на защите)
**Тема:** MongoDB sharded cluster для time-series телеметрии компрессоров.

**Цель:** показать, что кластерная архитектура (config server + 2 шарда с replica set + mongos)
обеспечивает:
1. загрузку и обработку данных на полном датасете;
2. горизонтальную масштабируемость;
3. выполнение распределённых аналитических запросов;
4. отказоустойчивость;
5. предсказуемую модель консистентности.

---

## 1) Что изменить в архитектуре прямо сейчас

### 1.1 Явно зафиксировать, что `COMP` — это идентификатор компрессора
По твоему CSV поле `COMP` и есть фактический `sensor_id` (идентификатор компрессора).
Значит проблема не в данных, а в согласованности названий в коде/запросах/презентации.

**Решение (выбери один стиль и держи его везде):**
- **Вариант A (проще):** оставить `metaField: "COMP"` и в запросах использовать `COMP`.
- **Вариант B (более читаемо на защите):** при импорте добавить alias `sensor_id = COMP`, использовать `metaField: "sensor_id"`, а `COMP` оставить как исходное поле.
- Для выбранного варианта одинаково настроить shard key (например, `{ COMP: 1, timestamp: 1 }` или `{ sensor_id: 1, timestamp: 1 }`).

**Эффект для защиты:** можно чётко объяснить семантику поля и избежать терминологической путаницы.

### 1.2 Сделать bootstrap идемпотентным
Скрипт инициализации должен запускаться повторно без падений.

**Решение:**
- перед `replSetInitiate` проверять, инициализирован ли replica set;
- перед `addShard` проверять, не добавлен ли шард;
- убрать жёсткие `sleep`, заменить ожиданием `PRIMARY` (polling `hello` / `replSetGetStatus`).

**Эффект:** проще демонстрировать, меньше случайных ошибок на показе.

### 1.3 Перенести конфигурацию в env
Вынести все параметры в переменные окружения:
- `MONGO_URI`, `DB_NAME`, `COLLECTION_NAME`, `CSV_PATH`, `BATCH_SIZE`.

**Эффект:** воспроизводимость экспериментов (можно быстро менять настройки и повторять замеры).

---

## 2) Что изменить в коде импорта

### 2.1 Убрать дублирование импортёров
Оставить один pipeline импорта:
- подготовка коллекции (опционально);
- чтение CSV чанками;
- преобразование типов;
- batch insert.

### 2.2 Ускорить загрузку
- `read_csv(..., parse_dates=["timestamp"], chunksize=N)`;
- без `iterrows()`, через `to_dict("records")`;
- `insert_many(records, ordered=False)`;
- логировать `rows/sec` и latency батча.

### 2.3 Добавить валидацию
Перед вставкой проверять:
- обязательные колонки;
- диапазоны значений (например, температура не `NaN`);
- долю отбракованных строк.

**Эффект для защиты:** можно показать качество данных и честно рассказать про cleaning.

---

## 3) Как закрыть критерии преподавателя (по пунктам)

## 3.1 Data import
Что показать:
- общий объём датасета, размер батча, время полной загрузки;
- среднюю скорость вставки (`docs/sec`);
- пример логов/метрик импорта.

Минимальная метрика в отчёт:
- `T_import_full`, `throughput_avg`, `failed_rows`.

## 3.2 Scalability
Сделать 2 режима:
1. **Без шардирования / 1 шард**
2. **С шардированием / 2 шарда**

Сравнить:
- время вставки;
- latency 2–3 аналитических запросов;
- распределение чанков (`sh.status()`, `balancer`).

Что сказать на защите:
- какая операция масштабируется почти линейно, а какая упирается в bottleneck.

## 3.3 Distributed queries
Подготовить 3 запроса:
1. moving average по `sensor_id`;
2. anomaly detection (фильтр по порогам + агрегация по окнам);
3. daily/hourly rollup.

Показать `explain("executionStats")`:
- что запрос идёт через `mongos`;
- что задействуются несколько шардов (где применимо);
- сколько документов прочитано/возвращено.

## 3.4 Availability
Сценарий демо:
- во время чтения/запросов остановить primary одного shard replica set;
- дождаться election;
- показать, что чтение продолжается (с корректным `readPreference`), запись восстанавливается после election.

Фиксировать:
- время деградации;
- факт восстановления.

## 3.5 Consistency model
Явно описать в проекте:
- для критичных записей: `writeConcern: { w: "majority" }`;
- для демонстрации доступности чтения: `readPreference: secondaryPreferred` (или `primary` там, где нужна строгая свежесть);
- для причинно-согласованного чтения при необходимости — сессии.

На защите проговорить trade-off:
- сильнее консистентность => выше задержка;
- выше доступность чтения => риск видеть не самые свежие данные.

---

## 4) Минимальный набор артефактов для отличной сдачи

1. **Архитектурная схема** (1 слайд): config server, shard1/2, реплики, mongos, importer.
2. **Таблица экспериментов** (1 слайд):
   - режим кластера,
   - объём данных,
   - import throughput,
   - query p50/p95,
   - поведение при отказе.
3. **2–3 explain-плана** распределённых запросов.
4. **Failover-демо** или скриншоты/логи с таймингом election.
5. **Чёткий вывод**: что получилось на full dataset и где ограничения.

---

## 5) План реализации без деления по дням (делаем всё)

Ниже единый чек-лист, который можно выполнять подряд и закрыть весь объём:

1. **Схема и кластер**
   - зафиксировать naming (`COMP` везде или alias `sensor_id = COMP`);
   - привести `metaField` и shard key к выбранному naming;
   - сделать `init_mongo.js` идемпотентным;
   - добавить readiness-check вместо фиксированных `sleep`.

2. **Импорт данных**
   - оставить один импортёр;
   - `read_csv(..., parse_dates=["timestamp"], chunksize=...)`;
   - вставка через `insert_many(..., ordered=False)`;
   - добавить валидацию обязательных полей и логирование ошибок.

3. **Метрики и reproducibility**
   - все параметры через env (`MONGO_URI`, `DB_NAME`, `COLLECTION_NAME`, `CSV_PATH`, `BATCH_SIZE`);
   - в логах фиксировать `T_import_full`, `throughput_avg`, `failed_rows`;
   - сохранить результаты в таблицу для слайдов.

4. **Distributed queries и explain**
   - подготовить 3 запроса (moving average, anomaly detection, rollup);
   - для каждого снять `explain("executionStats")`;
   - зафиксировать latency p50/p95.

5. **Scalability experiment**
   - прогнать одинаковые тесты в 2 режимах: 1 shard и 2 shards;
   - сравнить import throughput и query latency;
   - показать распределение чанков (`sh.status()`).

6. **Availability + consistency demo**
   - во время нагрузки уронить primary одного shard RS;
   - зафиксировать время election и восстановление операций;
   - явно показать выбранные `writeConcern`/`readPreference` и trade-off.

7. **Финальная упаковка для защиты**
   - архитектурная схема кластера;
   - таблица всех метрик/сравнений;
   - 2–3 explain-скрина;
   - failover-лог/скрин;
   - финальный вывод по feasibility и ограничениям.

## 6) Если времени мало — приоритеты
1. Согласованная схема + стабильный импорт.
2. Scalability benchmark (хотя бы 2 конфигурации).
3. 2 убедительных distributed query с explain.
4. 1 сценарий отказа и восстановление.

Этого уже достаточно, чтобы закрыть все обязательные критерии на хорошем уровне.
