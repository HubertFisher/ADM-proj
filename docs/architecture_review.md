# Рекомендации по улучшению архитектуры и кода

## 1) Убрать дублирование логики и разделить ответственности
Сейчас есть два импортера (`scripts/import.py` и `scripts/setup_import.py`) с частично пересекающимся поведением и разными источниками конфигурации. Рекомендуется:
- оставить один основной entrypoint импорта;
- вынести общую логику (подключение, валидация, batch insert) в модуль `scripts/lib/importer.py`;
- отдельно вынести bootstrap кластера и создание коллекции/шардирования.

## 2) Конфигурация только через переменные окружения
Сейчас URI/пути/имена БД захардкожены. Рекомендуется централизовать параметры:
- `MONGO_URI`, `DB_NAME`, `COLLECTION_NAME`, `CSV_PATH`, `BATCH_SIZE`;
- единый модуль конфигурации с дефолтами и валидацией;
- одинаковые значения локально и в Docker.

## 3) Явно зафиксировать mapping `COMP == sensor_id`
В текущем CSV поле `COMP` фактически является идентификатором компрессора (аналог `sensor_id`).
Риск не в самом поле, а в терминологии: если в части кода/запросов использовать `sensor_id`, а в схеме оставлять только `COMP`, легко получить путаницу.

Рекомендуется выбрать один из двух путей и применять его везде:
- **Вариант A (минимальные изменения):** оставить в схеме `metaField: "COMP"` и во всех запросах использовать `COMP`.
- **Вариант B (более читаемо):** при импорте создавать alias `sensor_id = COMP`, задавать `metaField: "sensor_id"`, а `COMP` хранить как исходное поле для трассируемости.

Главное — единообразно использовать выбранное имя в:
- создании time-series коллекции;
- shard key;
- импорте данных;
- аналитических запросах и слайдах защиты.

## 4) Добавить идемпотентный bootstrap и проверку готовности кластера
`init_mongo.js` использует `sleep(10000)`, что может быть нестабильно. Рекомендуется:
- перед `replSetInitiate` проверять состояние, чтобы можно было запускать повторно;
- заменить fixed sleep на polling `rs.status()`/`hello` до PRIMARY;
- добавить healthcheck в `docker-compose` и запуск импорта только после готовности mongos.

## 5) Улучшить производительность импорта
В `import.py` используется `iterrows()`, это медленно для больших CSV. Рекомендуется:
- использовать векторизованные преобразования и `to_dict('records')`;
- использовать `insert_many(..., ordered=False)`;
- включить `parse_dates=["timestamp"]` в `read_csv`;
- при необходимости писать в несколько потоков/процессов с ограничением нагрузки.

## 6) Добавить валидацию и наблюдаемость
Для production-стиля стоит добавить:
- проверку обязательных колонок CSV;
- базовую нормализацию типов и обработку пропусков/NaN;
- structured logging вместо `print`;
- метрики: скорость импорта (records/sec), размер батча, число ошибок.

## 7) Упростить Docker образ и воспроизводимость
Сейчас зависимости ставятся без pinning. Рекомендуется:
- добавить `requirements.txt` с фиксированными версиями;
- использовать multi-stage при необходимости;
- запускать импорт как отдельный job/command, а не как постоянный сервис.

## 8) Тесты и CI
Минимальный набор:
- unit-тесты на преобразование строки CSV в документ Mongo;
- smoke-тест, который проверяет создание коллекции и успешный batch insert;
- проверка линтерами (`ruff`, `black`) и запуск тестов в CI.

## 9) Структура проекта
Предложение по структуре:

```text
scripts/
  app/
    config.py
    importer.py
    bootstrap.py
    schema.py
  cli.py
  tests/
```

Это позволит масштабировать проект, добавить CLI-команды (`bootstrap`, `import`, `query`) и снизить связность.
